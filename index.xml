<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>My Data Science (y más) Portfolio on Carlos Francisco Mendoza Lara</title>
    <link>https://franciscomendozalara.github.io/cfml/</link>
    <description>Recent content in My Data Science (y más) Portfolio on Carlos Francisco Mendoza Lara</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 01 Nov 2021 10:58:08 -0400</lastBuildDate><atom:link href="https://franciscomendozalara.github.io/cfml/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>UrbanSound8k</title>
      <link>https://franciscomendozalara.github.io/cfml/projects/project-1/</link>
      <pubDate>Mon, 01 Nov 2021 10:58:08 -0400</pubDate>
      
      <guid>https://franciscomendozalara.github.io/cfml/projects/project-1/</guid>
      <description>Do you recognize this?
 That is how Dancing Queen by ABBA looks like if we compute and display its MEL-Spectrogram.
How about this one?
 That MEL-Spectrogram corresponds to Comfortably Numb by Pink Floyd.
The Mel-Frequency Cepstral Coefficients (MFCC) are mathematical coefficients for sound modelling. These coefficients are used as features for speech recognition and sound classification. And that is what I used as features to classify different urban sounds from 10 different classes.</description>
    </item>
    
    <item>
      <title>Cepstral analysis for acoustic vibrations</title>
      <link>https://franciscomendozalara.github.io/cfml/projects/project-2/</link>
      <pubDate>Mon, 18 Jan 2021 10:58:08 -0400</pubDate>
      
      <guid>https://franciscomendozalara.github.io/cfml/projects/project-2/</guid>
      <description>A common technique used for speech (and sound) analysis is Cepstral analysis. For discrete-time signals, the cepstrum is defined as the inverse discrete-time Fourier Transform of the natural logarithm of the discrete-time Fourier Transform of the signal.
 Human create speech signals through a series of controlled movements of their lungs, vocal cords, tongue, and lips. Speech can be separated into two sound types, voiced and unvoiced. Voiced speech has a roughly regular pattern in its time-frequency structure whereas unvoiced speach does not.</description>
    </item>
    
    <item>
      <title>Explore Linear Regression</title>
      <link>https://franciscomendozalara.github.io/cfml/projects/project-3/</link>
      <pubDate>Sun, 18 Oct 2020 11:00:59 -0400</pubDate>
      
      <guid>https://franciscomendozalara.github.io/cfml/projects/project-3/</guid>
      <description>This is an R Shiny app to explore how the linear regression model works.
Link to the app
Link to GitHub Repository
 </description>
    </item>
    
    <item>
      <title>Contact</title>
      <link>https://franciscomendozalara.github.io/cfml/contact/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://franciscomendozalara.github.io/cfml/contact/</guid>
      <description>   Platform URL     Email : francisco.mendoza.lara@gmail.com   LinkedIn: https://www.linkedin.com/in/francisco-mendoza-lara/   GitHub: https://github.com/FranciscoMendozaLara   YouTube: https://www.youtube.com/channel/UCykzHq6kNUewhyc-Lp0S97g    </description>
    </item>
    
  </channel>
</rss>

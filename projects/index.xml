<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects on Carlos Francisco Mendoza Lara</title>
    <link>https://franciscomendozalara.github.io/cfml/projects/</link>
    <description>Recent content in Projects on Carlos Francisco Mendoza Lara</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 18 Oct 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://franciscomendozalara.github.io/cfml/projects/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>UrbanSound8k</title>
      <link>https://franciscomendozalara.github.io/cfml/projects/project-1/</link>
      <pubDate>Mon, 01 Nov 2021 10:58:08 -0400</pubDate>
      
      <guid>https://franciscomendozalara.github.io/cfml/projects/project-1/</guid>
      <description>Do you recognize this?
 That is how Dancing Queen by ABBA looks like if we compute and display its MEL-Spectrogram.
How about this one?
 That MEL-Spectrogram corresponds to Comfortably Numb by Pink Floyd.
The Mel-Frequency Cepstral Coefficients (MFCC) are mathematical coefficients for sound modelling. These coefficients are used as features for speech recognition and sound classification. And that is what I used as features to classify different urban sounds from 10 different classes.</description>
    </item>
    
    <item>
      <title>Cepstral analysis for acoustic vibrations</title>
      <link>https://franciscomendozalara.github.io/cfml/projects/project-2/</link>
      <pubDate>Mon, 18 Jan 2021 10:58:08 -0400</pubDate>
      
      <guid>https://franciscomendozalara.github.io/cfml/projects/project-2/</guid>
      <description>A common technique used for speech (and sound) analysis is Cepstral analysis. For discrete-time signals, the cepstrum is defined as the inverse discrete-time Fourier Transform of the natural logarithm of the discrete-time Fourier Transform of the signal.
 Human create speech signals through a series of controlled movements of their lungs, vocal cords, tongue, and lips. Speech can be separated into two sound types, voiced and unvoiced. Voiced speech has a roughly regular pattern in its time-frequency structure whereas unvoiced speach does not.</description>
    </item>
    
    <item>
      <title>Explore Linear Regression</title>
      <link>https://franciscomendozalara.github.io/cfml/projects/project-3/</link>
      <pubDate>Sun, 18 Oct 2020 11:00:59 -0400</pubDate>
      
      <guid>https://franciscomendozalara.github.io/cfml/projects/project-3/</guid>
      <description>This is an R Shiny app to explore how the linear regression model works.
Link to the app
Link to GitHub Repository
 </description>
    </item>
    
  </channel>
</rss>
